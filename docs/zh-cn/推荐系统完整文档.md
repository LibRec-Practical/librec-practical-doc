# 一、整体架构
![图片](https://uploader.shimo.im/f/EZZL2AlOsvMtDYrw.png!thumbnail)

图 1：来自阿里云推荐引擎

![图片](https://uploader.shimo.im/f/PMqKWXt3hcUyjSc8.png!thumbnail)

图 2：推荐 API 集成(来自阿里云推荐引擎)

![图片](https://uploader.shimo.im/f/nlVfFLVSLOcFN2uf.png!thumbnail)

OTS(Open Table Service)为阿里云的 NoSQL 数据库，我们可以将其当做或者说以 HBase 来替代

图 3：算法流程(来自阿里云推荐引擎)

![图片](https://uploader.shimo.im/f/s86llOb1txU6dzJX.jpg!thumbnail)

![图片](https://uploader.shimo.im/f/7TalP8gAlKoOiiMH.jpg!thumbnail)

我们目前的架构

# 二、业务场景
1.猜你喜欢  

2.相关推荐

计算物品或内容相似度，用以满足用户延伸阅读、使用等需求。

3.热门推荐

通过点击、收藏、评论等用户行为计算出热门及排名，将其提供给用户。

4.Feed 流

5.搜索引擎推荐（用户画像）

# 三、评价体系
评估指标：点击率、点击人数比、人均点击次数、留存率、转化率等。

# 四、主要模块部分
推荐系统一般包括**展现子系统**、**日志子系统**和**算法子系统**三个部分。

![图片](https://uploader.shimo.im/f/PlH6dPnX5ywSrH3h.jpg!thumbnail)

 

# 五、整体推荐全流程
![图片](https://uploader.shimo.im/f/7TalP8gAlKoOiiMH.jpg!thumbnail)

全流程框架

* yarn解决多租户资源调度的难题
* flume解决数据传输的难题
* kafka提供发布订阅机制的消息队列
* zookeeper可以帮助用户完成主备的选举
* hive在hdfs的基础上提供了数仓的功能
* hbase则基于hdfs实现列式数据库

但就目前我们想做的事情来看，在数据量极大的情况下，使用hive作为查询引擎的话，hive的查询无法支持秒级查询，所以[Impala ](https://www.jianshu.com/p/257ff24db397)应该挺适合。（impala使用hive的元数据, 完全在内存中计算）

所以CDH平台也可以考虑使用

# 六、数据
## 6.1 数据平台建设
### 6.1.1 数据处理流程
整个数据处理的过程，分为 5 步：

![图片](https://uploader.shimo.im/f/GpX5sTK63SQjO2pY.jpg!thumbnail)

### 6.1.2 数据平台架构图


## 6.2 数据分类
目前根据阿里云，知乎，等公司的推荐引擎数据类别，我们将其分为三大类数据：

* 用户类数据
* 物品类数据
* 行为类数据

我们的数据可以分为**用户行为**、**物品信息**、**用户画像**以及**外部数据**

(1) 用户行为

用户行为数据最重要。

一方面，用户行为数据是训练模型中的一个重要数据来源。

另一方面，需要通过用户的行为反馈，才能知道推荐系统到底做得如何。

积累用户行为数据。

用户行为数据的作用：

1.使用用户行为数据训练模型

2.使用用户行为数据可视化分析问题

3.使用用户行为数据验证效果

(2) 物品信息

物品信息指推荐系统中能采集到的描述每一个内容的信息。

(3) 用户画像

传统的思路中，认为用户画像里面存储的实际还是用户的标签，但在很多实际场景中标签数量少、维度粗，可能根本不具备去给用户打标签的能力，会限制搭建推荐系统的思路。

而从深度学习的角度出发，用户画像中储存的并不是通常理解的“标签”，而是一个人的向量，深度学习是把人和物品做向量化，但这个向量是不可被理解的，即我们可能并不知道这个向量表示的是什么意思，当我们看到某个用户对应的向量，我们也不知道他是对体育、音乐或是娱乐感兴趣，但我们仍能够通过向量去为他推荐其感兴趣的内容。

(4) 外部数据（可暂时先不考虑引入）

购买阿里或者是腾讯的外部数据来充实用户画像，从而提高推荐系统的效果。

## 6.3 数据模型
### 6.3.1 页面浏览（PV）
对于 Web 端， 由于 URL（Uniform Resource Locator） 的明确性，在统计上相对简单。可根据正则对 URL 进行分类，进而统计出某类页面的 PV（page view）。

对于客户端，统计的方式和 Web 端比较相似。由于客户端不像 Web 端天然具备 URL，因此需要为页面伪造 URL。那么定义好 URL 后，只要 URL 变化了，即可算一次新的 PV。

PV 为传统衡量产品成功与否的指标，我们预计将其同下述 Event Model 集成在一起。也就是说可以在 Event 模型中完成 PV 统计的操作。

下面是一个简单的统计 PV 的 Event 事件接口

{

    "distinct_id": "2b0a6f51a3cd6775",

    "time": 1434556935000,

    "type": "track",

    "event": "PageView",

    "properties": { 

        "$ip" : "180.79.35.65",

        "user_agent" : "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.）",

        "page_name" : "网站首页",

        "url" : "www.demo.com",

        "referer" : "www.referer.com"

    }    

}

### 6.3.2 行为事件
对于行为事件，决定选择事件模型（Event 模型）来描述用户在产品上的各种行为，这将成为之后所有的接口和功能设计的核心依据。

已知知乎，神策数据等选择了使用 Event 模型来完成业务数据分析及推荐系统。

事件模型包括事件（Event）和用户（User）两个核心实体。这两类数据应该能够分别或者贯通起来参与具体的分析和查询。

**Event 实体**

1.Who

用户和设备的身份特征。即参与这个事件的用户是谁。

在数据接口中，使用 distinct_id 来设置用户的唯一 ID：

对于未登录用户，这个 ID 可以是 cookie、设备 ID 等匿名 ID；

对于登录用户，则应该使用后台分配的实际用户 ID。

同时，也应该考虑提供 track_signup 这个接口，在用户注册的时候调用，用来将同一个用户注册之前的匿名 ID 和注册之后的实际 ID 贯通起来进行分析。

2.When

埋点触发的时间（这个事件发生的实际时间）。

3.How

埋点发生时，用户使用的方式或者说当前的状态，例如网络是 4G 还是 Wifi，当前的 AB 实验命中情况等等。模型中 Who、When、How 由埋点 SDK 自动生成，埋点人员在绝大多数情况下不必关心这三个要素。包括用户使用的设备、使用的浏览器、使用的 App 版本、操作系统版本、进入的渠道、跳转过来时的 referer 等

目前想要预置以下字段来描述 How 的信息：

* $app_version：应用版本
* $city： 城市
* $manufacturer： 设备制造商，字符串类型，如"Apple"
* $model： 设备型号，字符串类型，如"iphone6"
* $os： 操作系统，字符串类型，如"iOS"
* $os_version： 操作系统版本，字符串类型，如"8.1.1"
* $screen_height： 屏幕高度，数字类型，如 1920
* $screen_width： 屏幕宽度，数字类型，如 1080
* $wifi： 是否 WIFI，BOOL 类型，如 true

4.Where

准确定位一个事件发生的位置，应当包含该事件**在产品中的发生位置**以及**用户使用产品时的空间位置**。

主要包含以下几个字段提供埋点设计者来做用户事件的定位。

发生位置：

* LogType type=1；//日志类型
* int32 id=2；//由埋点管理平台生成，作用类似大家通常用的 event_name 
* string url=3；//当前页面 url 
* ModulePath module=4；//该位置所处的模块，模块之间的嵌套以及模块在父模块中的位置

空间位置：（数据分析整个地域的偏好）

* ip //ip 地址，自动根据 ip 来解析相应的省份和城市
* GPS （比如发现用户突然身处青海、西藏，或许可以在电影推荐中加入公路片，旅行片，民族文化片）这个一定是在推荐引擎中实现，此处主要分析可能会用到的数据并对我们的系统有帮助的字段有哪些
* city
* province

除了 $city 和 $province 这两个预置字段以外，也可以自己设置一些其它地域相关的字段。例如，某个从事社区 O2O 的产品，可能需要关心每个小区的情况，则可以添加自定义字段“HousingEstate”；或者某个从事跨国业务的产品，需要关心不同国家的情况，则可以添加自定义字段“Country”。

5.What

描述用户所做的这个事件的具体内容，采集的内容由业务决定。

可以使用“event”这个事件名称，来对用户所做的内容做初步的分类。

由于是根据具体业务收集内容的要素，所以可以暂时只使用 event 作为预置字段。

下面是我们暂时对 event 的分类：

{

pageView：页面浏览

 itemView：物品曝光

 click：用户点击物品

 collect：用户收藏了某个物品

 uncollect：用户取消收藏某个物品

 search_click：用户点击搜索结果中的物品

 comment：用户对物品的评论

 share：分享

}

* 对于一个“购买”类型的事件，则可能需要记录的字段有：商品名称、商品类型、购买数量、购买金额、 付款方式等；
* 对于一个“搜索”类型的事件，则可能需要记录的字段有：搜索关键词、搜索类型等；
* 对于一个“点击”类型的事件，则可能需要记录的字段有：点击 URL、点击 title、点击位置等；
* 对于一个“用户注册”类型的事件，则可能需要记录的字段有：注册渠道、注册邀请码等；
* 对于一个“用户投诉”类型的事件，则可能需要记录的字段有：投诉内容、投诉对象、投诉渠道、投诉方式等；
* 对于一个“申请退货”类型的事件，则可能需要记录的字段有：退货金额、退货原因、退货方式等。
### 6.3.3 Event 的划分和字段设计原则
**Event 事件推荐在后端进行记录**，这是出于以下一些考虑：

1. 很多行为，如下单等，他们的很多字段在前端（App 和 Web 界面）是拿不到的。甚至有些行为，如用户线下消费等，前端根本就没有提供相应的功能，就更拿不到对应的数据。
2. 后端修改程序更加方便便捷，如果是在 App 端记录数据，则每次修改都需要等待 App 的发版和用户更新；
3. App 端收集数据会有丢失的风险，并且上传数据也不及时。App 端为了避免浪费用户的流量，一般情况下，都是将多条数据打包，并且等待网络状况良好以及应用处于前台时才压缩上传，因此，自然会造成上传数据不及时，很有可能某一天的数据会等待好几天才传到服务器端，这自然会导致每天的指标都计算有偏差。同时，由于 App 端可以缓存的内容有限，用户设备的网络连接等问题，App 端收集的数据目前也没有太好的手段保证 100%不丢失。

基于以上几点考虑，**除非某个行为只在前端发生，对后端没有任何请求，否则，我们建议永远只在后端收集数据**。

**Event 的划分原则**

对产品划分 Event，建议如下：

1. 为了节约使用成本，应该从需求出发，只记录那些会分析到的 Event，这一点是与传统的 PV 分析产品一个很大的不同。记录 Event 是为了详细地了解用户是如何使用产品的，对于暂时不会分析到的那些使用情况，可以暂时先不记录。
2. Event 的数量不应过多，对于一个典型的用户产品，Event 的数量以不超过 20 个为宜。当然，这个只是我们对事件设计的一些原则性的建议，系统本身并没有这方面的限制。一些类似的用户操作，可以合并成一个 Event。例如，假设某个产品比较关心对一系列商品分类页的访问情况，那么，并不意味着每个商品分类页点击就应该划分成一个单独的 Event，而是可以划分出一个单独的“商品分类页访问” Event，然后再将不同的分类以字段的形式进行记录。
3. Event 不仅局限于用户在 App、Web 界面等前端的操作和使用，一些其它类型的用户行为，例如用户的电话投诉、用户在线下接收服务、用户在线下商家进行消费等，如果能够获取到相应的数据，并且数据分析也会用到，则也可以作为相应的 Event。

**字段设计原则**

为每个 Event 进行字段设计，建议如下：

1. 先根据需求梳理推荐系统所需数据，以及数据分析中需要用到的指标和维度，然后再从推荐所需数据，指标和维度倒推需要在每个 Event 记录的字段。
2. 我们要做的平台首先应该是基于大数据的推荐系统，接下来是数据分析平台，然后接下来可以考虑做日志的大规模存储

所以，一些用不到的字段，例如 Cookie 的完整内容、后端请求返回码等，就没有必要作为一个 Event 的字段来进行记录和收集了。

3. 预置字段中已经有的字段，则建议尽量复用预置字段。对所有预置字段的说明，可以参看数据格式中的相应说明。
4. 某个 Event 的某个字段的设计一旦确定，则不要再修改它的类型和取值含义。例如，一开始对于"Buy"这个 Event，我们设计了一个数值类型的字段"Money"，描述这个购买行为对应的购买金额是多少元，然而后面我们期望把它改成分，那么我们建议不如废弃掉"Money"字段并且增加一个新的字段叫"MoneyByCents"，而不是去改变"Money"的含义。

Event 事件样例：

{

    "distinct_id": "123456",

    "time": 1434556935000,

    "type": "track",

    "event": "ViewProduct",

    "project": "ebiz_test",

    "time_free": true, //建议在导入历史数据时使用，SDK 采集的实时数据不建议使用

    "properties": {

        "$is_login_id":true,  //此参数请慎重使用，详细介绍请参考文档底部 8.1 $is_login_id 参数说明

        "$app_version":"1.3",

        "$wifi":true,

        "$ip":"180.79.35.65",

        "$province":"湖南",

        "$city":"长沙",

        "$user_agent":"Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_2 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/58.0.3029.113 Mobile/14F89 Safari/602.1",

        "$screen_width":320,

        "$screen_height":640,

        "product_id":12345,

        "product_name":"苹果",

        "product_classify":"水果",

        "product_price":14.0

    }

}

### 6.3.4 User 实体
**记录和收集 User Profile**

每个 User 实体对应一个真实的用户，用 distinct_id 进行标识，描述用户的长期属性（也就是 Profile），并且通过 distinct_id 与这个用户所从事的行为，也就是 Event 进行关联。

一般记录 User Profile 的场所，是用户进行注册、完善个人资料、修改个人资料等几种有限的场合，与 Event 类似，也建议**在后端记录和收集 User Profile**。

我们应该收集哪些字段作为 User Profile，也完全取决于**产品形态**以及**分析需求**。简单来说，就是在能够拿到的那些用户属性中，哪些对于分析有帮助，则作为 Profile 进行收集。

**字段记录在 Profile 还是 Event 的取舍**

有些时候，我们可能会纠结，某个与用户相关的字段是应该记录在 Profile 还是记录在 Event，一个基本的原则就是，**Profile 记录的是用户的基本固定不变的属性**，例如性别、出生年份（请注意，记录的不是年龄而是出生年份）、注册时间、注册地域、注册渠道等。

而还有一些字段，例如用户级别、设备类型、地域、是否是 Vip 等，虽然也是用户相关的字段，但是可能是会经常变化的，则应该在用户的某个 Event 发生的时候，作为 Event 的一个字段来进行记录。

用户画像详细信息请参见下方**用户画像**部分

### 6.3.5 Item 实体
在 Event-User 模型中，出于性能和可解释性等各方面的考虑，Event 是被设计为不可变的。

从逻辑上看似乎没有问题，因为 Event 代表的是历史上已经发生过的事件，一般来说不应该需要进行更新。

但是，在实际的应用过程中，并不一定是这么理想的状态。

如，在采集和分析中会发现：

* Event 实体中一些基本信息中会有许多是不断变化的
* 埋点采集中，发现某些 Event 在最初的阶段采集到的数据不完善。

这时，可通过 Item 实体对 Event-User 模型进行补充。

这里的所谓 Item，在严格意义上是指一个和用户行为相关联的实体，可能是一个商品、一部电影、一个视频、一部小说等等。

## 6.4 数据格式
### 6.4.1 数据整体格式
日志文件是一行一个 JSON，物理上对应一条数据，逻辑上对应一个描述了用户行为的事件，或是描述一个或多个用户属性的 Profile 操作。

记录一个 Event 及关联的 Properties。

数据样例 track：

{

    "distinct_id": "123456",

    "time": 1434556935000,

    "type": "track",

    "event": "ViewProduct",

    "project": "ebiz_test",

    "time_free": true, //建议在导入历史数据时使用，SDK 采集的实时数据不建议使用

    "properties": {

        "$is_login_id":true,  //此参数请慎重使用，详细介绍请参考文档底部 8.1 $is_login_id 参数说明

        "$app_version":"1.3",

        "$wifi":true,

        "$ip":"180.79.35.65",

        "$province":"湖南",

        "$city":"长沙",

        "$user_agent":"Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_2 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/58.0.3029.113 Mobile/14F89 Safari/602.1",

        "$screen_width":320,

        "$screen_height":640,

        "product_id":12345,

        "product_name":"苹果",

        "product_classify":"水果",

        "product_price":14.0

    }

}

数据样例 track_signup：

{

    "distinct_id":"12345",

    "original_id":"2b0a6f51a3cd6775",

    "time": 1434557935000,

    "type": "track_signup",

    "event": "$SignUp",

    "project": "ebiz_test",

    "properties": {

        "$manufacturer":"Apple",

        "$model": "iPhone 5",

        "$os":"iOS",

        "$os_version":"7.0",

        "$app_version":"1.3",

        "$wifi":true,

        "$ip":"180.79.35.65",

        "$province":"湖南",

        "$city":"长沙",

        "$screen_width":320,

        "$screen_height":640

    }

}

我们推荐系统的核心价值应当是计算用户最有可能消费的物品（即 Item），并将推荐结果推送到产品前端供用户消费。

我们的推荐系统应以 Item 数据为基础构建推荐物品的画像（高维向量），计算用户最有可能消费的物品或者相似物品。

未来会一步步实现 Web 后台管理系统对 Item 表等表，进行直接的管理，例如管理员可以对某条待推荐的 Item 进行封禁或者调权以优化推荐效果。



# 七、用户画像平台
（完整细节，请移步 [用户画像完整文档](https://shimo.im/docs/gVDqHqYW3rcCwQvj)）

用户画像是倾向于对同一类用户进行不同维度的刻画，对同一个电商的买家进行用户画像设计，就是将买家进一步细分和具象，如闲逛型用户、收藏型用户、比价型用户、购买型用户等。

![图片](https://uploader.shimo.im/f/duYT8x5K7JEastzN.jpg!thumbnail)

用户画像计算引擎

用户画像平台建模抽象为5个层级：

1．基础数据接入

用户画像平台需接入主要数据源，并支持用户通过平台提供的数据接入工具接入新数据源，以便支持定制化的建模；

2．IDMapping

建模过程中通过IDMapping可以拿到用户ID关联特征，比如同一个账号被几个手机设备登录过，同时我们也支持接入新的ID，比如在一些金融项目中接入了用户支付的ID编码。

3．画像标签

目前画像包含6大类，标签数量超过2300，也支持在接入数据源中定制新的标签，画像平台会自动完成标签的实时及离线过程。

4．用户特征

在利用画像平台获取规整后的数据后，可从中根据数据标签探索的结果提取所需建模维度，画像平台可以支持生产离线训练样本。

5．AI算法

依据实际业务需求场景以及模型效果匹配适合的算法输出模型结果，用户画像平台也可以支持评测数据生成等工作。

## 7.1 用户画像建模
### 7.1.1 用户基**本属性表**
从用户处所得到的**属性标签**以及**推算出来的标签**。

主要用来帮助了解用户的人口统计学方面的基本情况。

作用：

按用户的人口属性推荐，如生日推荐等。

**性别模型**

虽然我们可能能够从用户处直接得到性别的填写，但仍要使用算法计算出性别。

性别模型包含两个模型：

-用户性别判断

-孩子性别判断（用于电商、医疗健康情境下，用户购买婴儿用品的推荐或用户关于婴儿医疗健康知识的文章推荐）

| 用户性别 | 1男   0女   -1未识别   | 1、商品性别得分2、用户购买上述商品计算用户性别等得分3、最优化算法训练阀值，根据阀值判断 | 
|:----:|:----:|:----|:----:|
| 孩子性别 | 0 仅有男孩1仅有女孩2男女都有3无法识别 | 1、选择男孩女孩商品  2、确定用户购买商品的男女性别比例  3、训练阈值，判断孩子性别,同用户性别类似   | 

**用户忠诚度模型**

| 用户忠诚度   | 1忠诚型用户  2偶尔型用户  3投资型用户  4游览型用户  -1未识别   | 总体规则：判断+聚类算法  1、游览用户型：只游览不购买的  2、购买天数大于一定天数的为忠诚用户  3、购买天数小于一定天数，大部分是有优惠才购买的  4、其他类型根据购买天数，购买最后一次距今时间，购买金额进行聚类   | 
|----|----|----|
**手机相关标签模型**

对于手机营销参考意义较大

使用手机品牌: 最常用手机直接得到

使用手机品牌档次：根据档次维表

使用多少种不同的手机：手机登陆情况

更换手机频率（月份):按时间段看手机登陆情况

**IDMapping模型**

多端设备登录，统一ID

作用：

-识别用户，统一用户ID

-识别马甲账号

马甲是指一个用户注册多个账号

多次访问地址相同的用户账号是同一个人所有

同一台手机登陆多次的用户是同一个人所有

收货手机号相同的账号同一个人所有

### 7.1.2 用户访问**信息表**
根据客户访问的情况提取相关客户标签。

作用：

了解用户的访问总体情况，方便根据客户游览习惯做营销

| 最近一次APP/PC端访问日期、  最近一次APP/PC端访问使用操作系统、  最近一次APP/PC端访问使用浏览器、  最近一次访问IP地址、  最近一次访问城市、  最近一次访问的省份-----分析用户最近一次访问情况。  第一次APP/PC端访问日期、  第一次APP/PC端访问使用操作系统、  第一次APP/PC端访问使用浏览器、  第一次访问IP地址、  第一次访问城市、  第一次访问的省份-----分析用户第一次访问情况。  近7天APP/PC端访问次数、  近30天APP/PC访问次数、  近60天APP/PC端访问次数、  近90天APP/PC端访问次数、  近180天APP/PC端访问次数、  近365天APP/PC端访问次数----分析用户APP/PC端访问次数。  近30天PC/APP端访问天数、  近30天PC/APP端访问并购买次数、  近30天PC/APP端访问PV、  近30天PC/APP端访问平均PV、  近30天PC/APP端最常用的游览器、  近30天PC/APP端不同IP数、  近30天PC/APP端最常用IP-----分析用户访问详情。  近30天0-5点访问的次数、  近30天6-7点访问的次数、  近30天8-9点访问的次数、  近30天10-12点访问的次数、  近30天13-14点访问的次数、  近30天15-17点访问的次数、  近30天18-19点访问的次数、  近30天20-21点访问的次数、  近30天22-23点访问的次数----分析用户喜欢在哪个时间上网访问。   | 
|----|
### 7.1.3 用户观看/访问类目表
根据用户观看/访问类目的日志记录来提取用户标签

作用：

-了解类目的观看/访问人群情况和针对某一类目的推荐等。

类目标签：

| 一级分类ID、一级分类名称、二级分类ID、二级分类名称、三级分类ID、三级分类名称-----分析用户都购买了哪些类目。 | 
|----|
### 7.1.4 用户购买类目表
根据用户购买类目的情况提取用户标签

作用：

-了解类目的购买情况和针对某一类目的推荐等。

类目标签：

| 一级分类ID、一级分类名称、二级分类ID、二级分类名称、三级分类ID、三级分类名称-----分析用户都购买了哪些类目。 | 
|----|
 








### 数据来源
用户画像的数据来源主要包括两个方面：

* 属性数据

这部分数据一般是用户的注册信息，也可以是从其他数据中分析得出的。比如生日、性别、住址、爱好等

* 行为数据

这部分数据一般都是用户的访问日志记录的行为数据。比如常用的一些后端日志数据、前端埋点数据等等。

通过这两部分的数据，就能刻画出用户的画像，其实就是给用户打上一个标签，比如军事爱好者、科幻狂热粉等等。

目前，网站主要是根据用户的注册ID信息获取用户的基本信息的，包括用户的年龄、职业、地域等，并通过检测浏览器Cookie和IP地址的方式记录用户的浏览信息，对用户进行分析，解析用户人群画像。


### 定量画像与定性画像
1. 定量画像：
  1. 以可以定量表示的数值标签构成，如Age：18， Money：30000等
  2. 定量画像需要确定标签的颗粒度。
  3. 无法通过数值表示如用户兴趣模型等
2. 定性画像：（更为重要）
  1. 以语义化、短文本为特征的标签构成，如 收藏：“泰坦尼克号” 、“蜘蛛侠”
### 用户画像表示方法
用户画像有很多中表示的方法，比如：

* **关键词法**，用一组关键词描述画像，这种方式最容易理解
* **评分矩阵法**，用二维矩阵的方式，通过评分表示。这种在协同过滤中很常见
* **向量空间表示法**，这种是基于向量的方式描述画像
* **本体表示法**
### 用户画像存储
不同的表示方法，对于存储的方式来说，也会不同。

一般用户画像常用的存储引擎有：

* 关系型数据库，通过中间表的方式存储用户和画像之间的关系，有点就是查询快、缺点就是数据量大的时候扛不住。
* NoSql数据库，常见的就是键值数据库（Redis）列数据库（Hbase）文档数据库（mongodb）图数据库（neo4j）等，每种数据库都有自己适合的场景。
* 数据仓库，数据仓库是一种面向主题的存储,可以更好的解决领域分析的问题。而且它保留了数据的时间变化，对于分析历史来说，非常有帮助。但是不适合实时查询。
### 用户画像查询
用户画像的场景下：

* 1 对于画像的查询要求是高并发、高聚合，比如会有很多人查询它自己的画像；也会通过一个画像属性，查询所有相关的人。
* 2 在画像的查询中，大量的查询都是重复的，因此可以着重考虑缓存机制
## 群体用户画像
## 构建步骤
1. 用户画像获取
2. 用户画像相似度计算
  1. 定量相似度计算：![图片](https://uploader.shimo.im/f/qUtVwZugzHceiRvJ.png!thumbnail)
    1. ui、uj为用户， sim(pk,pk)为两个用户在第k个标签的相似度，wk为该标签的权值
    2. 考虑到不同标签的定义域范围不同，需要对标签进行归一化处理(0,1)区间中。
    3. sim函数为自主选择的距离计算公式（欧氏距离、曼哈顿距离等）
  2. 定性相似度计算（映射到定量标签或者基于概念的相似度计算）
    1. 定量映射：云模型
    2. 基于概念概念：1. 本体 2.语料库
3. 用户画像聚类
  1. 聚类算法 层次化聚类算法、划分式聚类算法、基于密度和网格的聚类算法
  2. ![图片](https://uploader.shimo.im/f/yJBVmuL7FTICMcpW.png!thumbnail)

## 用户画像的管理
1. 用户画像的存储
  1. 关系型数据库：至少需要三个表，其中一个表用来存储用户，第一个表存储用户标签，最后一个表存储用户与标签的对应关系。
  2. 非关系型数据库
    1. 由于用户画像数据的特点及查询需求，用户画像数据的存储主要使用列数据库及Key-Value数据库。
  3. 数据仓库
1. 用户画像的查询
  1. 在涉及用户画像的大量查询中，通常有很多查询都是重复的，可以通过构建查询缓存机制提高查询效率
  2. 用户画像的查询强调极高的查询并发性能，进行查询分解，从而设计并行查询，对一些高负荷大数据量数据进行分治处理，每个部分的查询并发地运行，进-步会将各个部分的结果组合起来，提供最终的查询结果。
1. 用户画像的更新
  1. 用户画像更新的触发条件设置
    1. 第一种方式通过设置一个阈值，当获取的实时画像数据量超过这一阈值时，根据存储的画像数据构建用户画像.
    2. 第二种方式则设置一个时间周期，每隔该周期时间根据存储的画像数据构建用户画像.
    3. 第三种方式首先从增加的数据中挖掘用户画像，然后将其与原先得到的用户画像进行比较，根据比较的结果来决定是否更新。不同的方式有不同的适应场景，第一种方式适合数据敏感型的用户画像，第二种方式则适合时效性要求较高的用户画像，而第三种方式适合相对稳定的用户画像。
  2. 用户画像更新的机制
    1. 增量更新 滑动窗口更新法 增加触发器
    2. 对于定量标签， 更新标签中的值即可
    3. 对于定性标签，可以采用关联的方式进行更新
    4. ![图片](https://uploader.shimo.im/f/gsoKjmVFu0wccqEi.png!thumbnail)



# 推荐引擎
## 算法部分
整个推荐系统将包含多个模型。

将其分为召回、排序、规则三个步骤。首先召回用户感兴趣的内容，第二为用户生成一个排序列表，第三用规则解决一些产品、运营方面提出的需求。

## 召回
1.标签

基于标签的方法。基于人口统计学的推荐，基于内容标签的推荐。

标签可以分为两种：一是用户标签；二是内容标签。

2.协同过滤

兴趣相投，拥有共同经验的人群

3.深度学习

1) 深度学习召回新文章

使用深度学习为每篇新文章训练得到一个语义向量

2) 基于用户行为的深度学习召回模型

将推荐问题视为一个包含数百万类的多分类问题，以某一时刻用户的浏览序列作为模型输入，预测下一刻用户可能会浏览的内容。

我们使用深度学习模型解决多分类问题，从数百万的可推荐内容中召回可能感兴趣的内容。

深度学习召回比协同过滤的优势：

1.更全面的行为表达

在模型中结合点击、收藏、搜索等多种行为，能更全面地表示用户行为偏好，而这在协同过滤中不支持。

2.可添加画像特征，如加入性别、地域等用户画像相关的特征

如果企业有额外的标签或发生的信息数据在模型中可以兼容，支持把所有信息霖杂在同一模型里面进行分析，而在协同过滤中完全无法支持。

3.考虑用户的行为顺序

比如用户通常的行为顺序是，先买一个手机，然后再买手机壳；先买汽车后再买汽车坐垫。

如果推荐顺序是：用户买手机壳后被推荐了一款手机，那么逻辑就错误了。

协同过滤不具备序列关系的学习能力，它将所有的行为都看成一个完全平等的关系，而深度学习模型具备序列关系学习能力，在整体模型的表达的能力和调优方面都非常前沿。

### 粗召回



### 精召回

## 排序
**1.GBDT+LR 排序模型**

GBDT+LR 模型将 GBDT 模型和 LR 模型组合到了一起，可以将 GBDT 看做是对特征做离散化和组合编码的过程，最后的 LR 是最终的预测模型。

GBDT+LR 模型具有较强的学习能力，实施成本较低，对所需训练数据量和机器要求较低，具有较强的可解释性，当客户的训练数据量有限时通常是第一选择。

**2.Wide&Deep 排序模型**

Wide&Deep 模型包括 Wide 和 Deep 两部分的结构。

Wide 部分使用的是 LR 模型，主要负责记忆，学习特征间的相关性，生成的推荐结果是和已经有用户行为的物品直接相关的物品；

Deep 部分使用的是深度神经网络结构，主要负责泛化，学习新的特征组合，提高推荐结果的多样性。

**3.DeepFM 排序模型**

DeepFM 是一个集成了 FM 和 DNN 的神经网络框架，和 Wide&Deep 相似，都包括 Wide 和 deep 两部分，DeepFM 的 Wide 部分则是 FM 模型，Deep 部分是深度神经网络。DeepFM 的 Wide 和 deep 部分共享相同的输入，可以提高训练效率，不需要额外的特征工程，用 FM 建模低维的特征组合，用 DNN 建模高维的特征组合，因此可以同时从原始特征中学习到低维和高维的特征组合。

























